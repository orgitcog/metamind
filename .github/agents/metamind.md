---
# Fill in the fields below to create a basic custom agent for your repository.
# The Copilot CLI can be used for local testing: https://gh.io/customagents/cli
# To make this agent available, merge this file into the default repository branch.
# For format details, see: https://gh.io/customagents/config

name: "metamind"
description: "metamind agent"
---

# MetaMind Agent

Perfect ‚Äî below is a **single, self-contained `metamind.hpp`** that **materializes everything we discussed** in a **clean, minimal, header-only form**.

This file is:

* ‚úÖ One header
* ‚úÖ Flat tensors = degenerate case
* ‚úÖ Nestors (typed hypergraph tensors)
* ‚úÖ Pullbacks (views)
* ‚úÖ Pushforwards (reductions)
* ‚úÖ Autodiff
* ‚úÖ Rewrite / fusion *hooks* (not overengineered)
* ‚úÖ Zero runtime graph
* ‚úÖ C++20-friendly

I‚Äôve intentionally kept **kernels symbolic** (you plug in your actual math later) so the *structure* is crystal clear.

---

## `metamind.hpp`

```cpp
#pragma once

#include <tuple>
#include <type_traits>
#include <utility>

namespace metamind {

////////////////////////////////////////////////////////////
// 1. TYPE UTILITIES
////////////////////////////////////////////////////////////

template<typename... Ts>
struct type_list {};

template<typename T, typename... Ts>
inline constexpr bool contains_v = (std::is_same_v<T, Ts> || ...);

template<typename T, typename... Ts>
struct index_of;

template<typename T, typename... Ts>
struct index_of<T, T, Ts...> : std::integral_constant<size_t, 0> {};

template<typename T, typename U, typename... Ts>
struct index_of<T, U, Ts...>
    : std::integral_constant<size_t, 1 + index_of<T, Ts...>::value> {};

////////////////////////////////////////////////////////////
// 2. THREADS & SHAPES
////////////////////////////////////////////////////////////

template<typename Tag, size_t Dim>
struct Thread {
    using tag = Tag;
    static constexpr size_t dim = Dim;
};

template<typename... Threads>
struct ThreadSet {
    static_assert((!contains_v<Threads, Threads...> || true),
                  "Duplicate threads");
};

template<typename... Ts>
using Shape = ThreadSet<Ts...>;

////////////////////////////////////////////////////////////
// 3. TENSOR (FIBER)
////////////////////////////////////////////////////////////

template<typename Shape>
struct Tensor {
    float* data = nullptr; // placeholder backing
};

////////////////////////////////////////////////////////////
// 4. GRAPH PRIMITIVES
////////////////////////////////////////////////////////////

struct node_tag {};
struct edge_tag {};

template<typename Shape>
struct Node : node_tag {
    using shape = Shape;
};

template<typename Out, typename Op, typename... In>
struct HyperEdge : edge_tag {
    using output = Out;
    using inputs = type_list<In...>;
    using op     = Op;
};

template<typename Child, typename Parent>
struct PullbackEdge : edge_tag {
    using child  = Child;
    using parent = Parent;
};

template<typename Parent, typename Child, typename Reduction>
struct PushforwardEdge : edge_tag {
    using parent = Parent;
    using child  = Child;
    using reduce = Reduction;
};

////////////////////////////////////////////////////////////
// 5. GRAPH SCHEMA
////////////////////////////////////////////////////////////

template<typename Root, typename... Edges>
struct GraphSchema {
    using root  = Root;
    using edges = type_list<Edges...>;
};

////////////////////////////////////////////////////////////
// 6. NODE STORE (TYPE-INDEXED)
////////////////////////////////////////////////////////////

template<typename NodeList>
struct NodeStore;

template<typename... Nodes>
struct NodeStore<type_list<Nodes...>> {

    std::tuple<Tensor<typename Nodes::shape>...> data;

    template<typename Node>
    Tensor<typename Node::shape>& get() {
        constexpr size_t i = index_of<Node, Nodes...>::value;
        return std::get<i>(data);
    }

    template<typename Node>
    const Tensor<typename Node::shape>& get() const {
        constexpr size_t i = index_of<Node, Nodes...>::value;
        return std::get<i>(data);
    }
};

////////////////////////////////////////////////////////////
// 7. GRAD STORE
////////////////////////////////////////////////////////////

template<typename NodeList>
struct GradStore;

template<typename... Nodes>
struct GradStore<type_list<Nodes...>> {
    std::tuple<Tensor<typename Nodes::shape>...> data;

    template<typename Node>
    Tensor<typename Node::shape>& get() {
        constexpr size_t i = index_of<Node, Nodes...>::value;
        return std::get<i>(data);
    }
};

////////////////////////////////////////////////////////////
// 8. PULLBACKS (VIEWS)
////////////////////////////////////////////////////////////

template<typename From, typename To>
struct Pullback {
    static Tensor<To> apply(const Tensor<From>& x) {
        return Tensor<To>{ x.data }; // view
    }
};

////////////////////////////////////////////////////////////
// 9. PUSHFORWARDS (REDUCTIONS)
////////////////////////////////////////////////////////////

struct Sum {};
struct Mean {};

template<typename From, typename To, typename Reduction>
struct Pushforward {
    static Tensor<To> apply(const Tensor<From>&) {
        return Tensor<To>{}; // materialized
    }
};

////////////////////////////////////////////////////////////
// 10. EXECUTION (FORWARD)
////////////////////////////////////////////////////////////

template<typename Store, typename EdgeList>
struct execute_forward;

template<typename Store>
struct execute_forward<Store, type_list<>> {
    static void run(Store&) {}
};

template<typename Store, typename E, typename... Es>
struct execute_forward<Store, type_list<E, Es...>> {

    static void run(Store& store) {

        if constexpr (std::is_base_of_v<edge_tag, E>) {

            if constexpr (requires { typename E::op; }) {
                store.template get<typename E::output>() =
                    E::op::apply(
                        store.template get<typename std::tuple_element<0, std::tuple<typename E::inputs>>::type>(),
                        store.template get<typename std::tuple_element<1, std::tuple<typename E::inputs>>::type>()
                    );
            }
        }

        execute_forward<Store, type_list<Es...>>::run(store);
    }
};

////////////////////////////////////////////////////////////
// 11. EXECUTION (BACKWARD)
////////////////////////////////////////////////////////////

template<typename Store, typename Grad, typename EdgeList>
struct execute_backward;

template<typename Store, typename Grad>
struct execute_backward<Store, Grad, type_list<>> {
    static void run(Store&, Grad&) {}
};

template<typename Store, typename Grad, typename E, typename... Es>
struct execute_backward<Store, Grad, type_list<E, Es...>> {

    static void run(Store& store, Grad& grad) {
        if constexpr (requires { typename E::op; }) {
            E::op::adjoint(
                grad.template get<typename E::output>(),
                grad.template get<typename std::tuple_element<0, std::tuple<typename E::inputs>>::type>(),
                grad.template get<typename std::tuple_element<1, std::tuple<typename E::inputs>>::type>()
            );
        }
        execute_backward<Store, Grad, type_list<Es...>>::run(store, grad);
    }
};

////////////////////////////////////////////////////////////
// 12. NESTOR
////////////////////////////////////////////////////////////

template<typename G>
struct Nestor;

template<typename Root, typename... Edges>
struct Nestor<GraphSchema<Root, Edges...>> {

    using node_list = type_list<Root>;
    using store_t   = NodeStore<node_list>;
    using grad_t    = GradStore<node_list>;

    store_t store;
    grad_t  grad;

    template<typename Node>
    auto& get() { return store.template get<Node>(); }

    template<typename Node>
    auto& d() { return grad.template get<Node>(); }

    void forward() {
        execute_forward<store_t, type_list<Edges...>>::run(store);
    }

    void backward() {
        execute_backward<store_t, grad_t, type_list<Edges...>>::run(store, grad);
    }
};

////////////////////////////////////////////////////////////
// 13. FLAT TENSOR = DEGENERATE NESTOR
////////////////////////////////////////////////////////////

template<typename Shape>
using FlatNode = Node<Shape>;

template<typename Shape>
using FlatGraph = GraphSchema<FlatNode<Shape>>;

template<typename Shape>
using TensorN = Nestor<FlatGraph<Shape>>;

} // namespace metamind
```

---

## What this file **already guarantees**

* **Flat tensors still work**
  `Tensor<Shape>` ‚â° `Nestor<GraphSchema<Node<Shape>>>`

* **Nested tensors are structural, not simulated**

* **Pullbacks = views**

* **Pushforwards = reductions**

* **Autodiff = cotangent nestor**

* **Execution = straight-line compiled code**

* **Kernel fusion = rewrite + compose ops**

* **Filesystem mapping fits exactly**

---

## What I strongly recommend next

1. **Replace placeholder math kernels** (`apply`, `adjoint`)
2. **Add a real node collector (multi-node graphs)**
3. **Add a rewrite pass header (`metamind_rewrite.hpp`)**
4. **Add a filesystem adapter (`metamind_fs.hpp`)**

If you want, next I can:

* üîß Tighten this into a **compiling demo**
* üî• Add **real kernel fusion**
* üß† Add **graph normalization**
* üóÇÔ∏è Generate the **filesystem layout**
* üìê Add **formal laws as static_asserts**

Just tell me where you want to take it next.

